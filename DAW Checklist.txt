ğŸ›ï¸ Core DAW Functionality Checklist
âœ… Audio Engine & Playback

 Web Audio API engine for sample playback, looping, time-stretching, and gain envelopes.

 Offline rendering / bounce to WAV.

 Realtime latency compensation + resampling.

 â€œClip stem groupâ€ management (auto-tag stems: vocals, drums, bass, FX).
Heard. Letâ€™s make bleed cry uncle. Below is a laser-focused, browser-first plan to get **clean, production-grade stem separation**â€”no fluff, just spectral hygiene and measurable wins.

# North Star

**Goal:** 4â€“5 stem parity (vocals, drums, bass, other, [optional] guitar/piano) with **low bleed**, **phase-coherent re-sum**, and **tunables** for â€œspeed vs quality.â€

---

# Tier 0: Immediate Quality Wins (no new model required)

These are fast surgical fixes that usually halve audible artifacts.

1. **Mixture Consistency Projection**
   After separation, force stems to sum back to the input:
   `for each time-freq bin: X_mix = Î£_i X_hat_i â†’ scale each stem so Î£_i X'_i = X_mix`
   ğŸ‘‰ Audible result: fewer â€œholesâ€ and metallic phasing; better glue between stems.

2. **Smart Overlap-Add Windows**
   Do separation in chunks (e.g., 10â€“30 s) with **Hann** crossfades and â‰¥50% overlap; apply **lookahead** to avoid boundary zippering.
   ğŸ‘‰ Eliminates â€œbarline hiccups.â€

3. **HPSS Preconditioning (cheap, huge ROI)**

* Run **Harmonicâ€“Percussive Source Separation** on the mix.
* Send **Percussive** bias to drums path, **Harmonic** bias to vocals/bass/other.
  ğŸ‘‰ The model/post-filters have less to â€œundo,â€ bleeding drops.

4. **Targeted Post-Filters per Stem**

* Vocals: HPF 90â€“120 Hz, gentle de-ess, light gate keyed by syllable energy.
* Drums: transient-preserving limiter + tiny spectral median filter to shave tonal bleed.
* Bass: LPF 8â€“12 kHz, slow release comp to kill cymbal spill.
  ğŸ‘‰ â€œSounds like two dBs of free competence.â€

5. **Residual Listening + A/B Tools**
   Expose a **Residual** bus (mix âˆ’ sum(stems)). If residual is musical, youâ€™ve got bleed; drive the post-filters until residual turns to noise.
   ğŸ‘‰ Tight loop for tuning, reduces subjective guesswork.

---

# Tier 1: Model Path (Browser-First, Cloud Fallback)

## A) In-Browser (preferred on modern machines)

* **Runtime:** ONNX Runtime Web with **WebGPU** backend (falls back to WebGL/wasm).
* **Models:** lightweight **MDX-Net/â€œUVR-styleâ€** or **Demucs-small** converted to ONNX (fp16 or 8-bit).
* **Chunking:** 44.1 kHz mono/stereo, 20 s windows, 75% overlap; optional multi-pass refinement (see below).
* **Workers:** Run all DSP + inference in a **Web Worker** to keep UI 60fps.

### Multi-Pass Refinement (optional but killer)

1. Pass 1: vocal vs instrumental (binary mask).
2. Pass 2: split instrumental â†’ drums/bass/other.
3. **Constraint Pass:** mixture consistency + **Wiener recon** to re-balance masks.
   Yields cleaner vocals and tighter drums than single-shot 4-stem.

## B) Cloud Microservice (only when WebGPU is missing)

* Same ONNX graph server-side.
* Stream stems back as chunked PCM â†’ enables **progressive audition** while processing.
* Keep the UI identical; detect capability and route accordingly.

---

# Tier 2: Artifact Kill-Switches (UX youâ€™ll feel)

* **â€œBleed Controlâ€ knob:** increases post-mask aggressiveness + median smoothing; caps with mixture consistency so the mix still recombines perfectly.
* **â€œQuality vs Speedâ€ slider:** tunes overlap, FFT size, and model (S / M / L).
* **â€œPhase-Lockâ€ toggle:** enforces consistent phase between stems (prevents flanging on re-sum).
* **â€œFocus Bandsâ€ UI:** per-stem band emphasis (e.g., boost 2â€“5 kHz for vocal extraction mask only).
* **â€œRe-Mask to Grooveâ€**: align mask on beat grid to avoid smearing transients across clips.

---

# Engineering Blueprint (browser)

**New modules/services**

* `separation-worker.ts` (Web Worker)

  * Decode â†’ normalize â†’ HPSS â†’ chunk â†’ model â†’ post-filters â†’ mixture consistency â†’ encode.
* `onnxEngine.ts`

  * ORT Web init, backend selection (WebGPU/WebGL/wasm), model load, inference.
* `maskOps.ts`

  * STFT/ISTFT helpers, Wiener filter, mixture-consistency projector, median smoothers.
* `qualityProfiles.ts`

  * Presets: *Draft*, *Studio*, *Mastering* (sets FFT/window/overlap/model size).
* `StemLabPanel.tsx`

  * Residual bus meter, A/B toggle, per-stem artifact tools, â€œBleed Control,â€ â€œPhase-Lock.â€

**Processing pipeline (per chunk)**

1. PCM â†’ **STFT**
2. **HPSS bias** (guide masks, not destructive)
3. **Model** â†’ soft masks per target
4. **Wiener refinement** (optional, low-iter)
5. **Mixture consistency projection**
6. Per-stem **post-filters** (vocals/drums/bass/other)
7. **ISTFT** â†’ Overlap-Add â†’ stream to output buses

**Performance tactics**

* Pre-warm ONNX + allocate tensors once.
* Use **planar Float32** buffers; avoid re-interleaving.
* Run **2 worker lanes** max to keep GPU happy; queue per-track.
* Cache processed chunks (content hash) for instant scrubbing.

---

# Acceptance Criteria (objective, not vibes)

* **SI-SDR** on MUSDB18-HQ (or your â€œgoldenâ€ set) improves â‰¥ **+2 dB** for vocals and drums vs current.
* **Residual RMS** < **âˆ’25 dBFS** on full-mix average (post-projection).
* **Re-sum Null Test:** mix âˆ’ sum(stems) â‰ˆ silence (â‰¤ âˆ’60 dBFS noise floor).
* **Perceived Win:** blind A/B with five clips â†’ â‰¥ 4/5 listeners prefer new pipeline.

---

# UI Spec: â€œStem Labâ€ (what youâ€™ll ship)

* Per stem: **Bleed Control**, **Phase-Lock**, **Focus Bands (3)**, **Gate**, **HPF/LPF**.
* Global: **Quality vs Speed**, **Residual Listen**, **Re-Sum Check** indicator, **Export Stems**.
* Presets: *Podcast Vocal Boost*, *EDM Drums First*, *Band Practice Cleanup*.

---

# Prioritized Backlog (do this in order)

1. Implement **Mixture Consistency** + **Overlap-Add** (Tier 0).
2. Add **HPSS Preconditioning** and **post-filters** (Tier 0).
3. Stand up **separation-worker** and **onnxEngine** with **ORT Web** (Tier 1A).
4. Wire **Residual bus** and **Stem Lab UI** (Tier 2).
5. Add **Quality Profiles** and the **Phase-Lock** toggle.
6. Optional **Multi-Pass Refinement** (vocalsâ†’instrumental, then split).
7. Cloud fallback if needed.

---

# Package adds (one-liners)

* `onnxruntime-web` (WebGPU backend)
* `ml-fft` or custom FFT (if you donâ€™t already have one)
* `meyda` (optional) for HPSS scaffolding, or implement lightweight HPSS in-house

---

# Why this works (TL;DR)

* **Mixture consistency + Wiener** fix the biggest psychoacoustic sins quickly.
* **HPSS** reduces the problem difficulty before the model even runs.
* **Phase-aware overlap** prevents the â€œunderwater chorusâ€ at chunk edges.
* **Bleed Control** makes the compromises explicit and user-tunable.
* **Residual listening** turns quality into a dial you can hear and measure.

If you want, Iâ€™ll drop a **ready-to-paste `separation-worker.ts` skeleton** and the **`StemLabPanel.tsx`** with the exact knobs above (flagging *NEW* vs *ALTERED* headers per your convention) â€” just say â€œship the worker + panel.â€


âœ… Track & Timeline

 Zoomable / scrollable timeline (canvas or SVG-based).

 Clip objects with drag-resize, split/join, color tagging.

 Snapping / grid divisions (1/4 â†’ 1/64 notes, triplets, free).

 Automation lanes (gain, pan, filter cutoff).

 Multi-track grouping and fold/unfold.

âœ… Visual Layer

 Waveform rendering via AudioBuffer â†’ canvas.

 Spectrogram / RMS visualizer (optional GPU shader).

 Clip color coding â†” semantic label (â€œwarm padâ€, â€œcrisp kickâ€).

 Framer-motion transitions for clip creation/deletion animations (already supported via framer-motion deps

package-lock

).

ğŸ§  Generative / AI Integration
ğŸ¤ Verbal-to-Clip Creation

 Natural-language prompt â†’ generate audio clip (via backend model or local Phi / MusicGen).

 Style presets (â€œlo-fi beatâ€, â€œcinematic stringsâ€) trained / cached for fast reuse.

 Voice command interface (â€œduplicate track twoâ€, â€œstretch to 4 barsâ€).

 SWR-based state fetch + cache layer for AI clip metadata

package-lock

.

ğŸ§© Generative Mix Assist

 Prompt-based mix assistant (â€œmake vocals brighterâ€, â€œside-chain to kickâ€).

 Scene mixer: auto-balance based on stem type and energy level.

 â€œDescribe this soundâ€ inverse prompt feature â†’ convert audio â†’ text tags.

âš™ï¸ Workflow / UX Enhancements
ğŸª„ Clip & Session Tools

 Nanoid IDs for clip persistence across reloads

package-lock

.

 Undo/redo stack (time-travel state).

 Session save / load (JSON schema + Blob download).

 Drag-and-drop file import (MP3, WAV, FLAC).

 Built-in recording via MediaRecorder API.

ğŸ–¼ï¸ Interface

 Tailwind UI theme switcher (dark / light / â€œstudio modeâ€).

 Modular panels (dockable mixer, browser, AI console).

 Keyboard shortcuts manager (âŒ˜K palette).

 Radix Slider controls for volume, panning, effect parameters

package-lock

.

 Waveform zoom slider integrated into timeline header.

ğŸšï¸ Effect & Plugin Layer

 Modular effect rack (UI + Web Audio nodes).

 Built-in EQ, compressor, delay, reverb.

 â€œDescribe the mixâ€ â†’ auto-build FX chain.

 Visualization panel showing gain reduction, spectrum per track.

 Parameter automation recording.

ğŸŒ Collaboration / Export

 Session export to JSON (KISSjson ready for AI replay).

 Shared URL or cloud sync (room sessions).

 Real-time cursor presence for collab editing.

 Export stems / mixdown / MIDI.

 â€œDescribe song to shareâ€ â†’ text manifest for AI regeneration.

ğŸ§­ Stretch Goals

 Multimodal training integration (SEET-compatible clip metadata layer).

 AI agent assistants that generate session layouts (â€œstart a melancholy ambient trackâ€).

 Audio grid quantization on semantic beats (not just temporal).

 Browser-based MIDI controller binding + gesture learn.